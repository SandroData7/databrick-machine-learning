# ðŸ“˜ IntegraciÃ³n ADLS Gen2 con Azure Databricks usando Access Connector + Unity Catalog

## ðŸ“Œ Resumen

Este documento describe cÃ³mo conectar Azure Databricks (Unity Catalog) con un Data Lake Gen2 utilizando:

- Access Connector for Azure Databricks
- Managed Identity
- Storage Credential
- External Data
- Unity Catalog schema

Este mÃ©todo reemplaza `dbutils.fs.mount` y cumple con los requisitos de seguridad de UC.

---

## âœ… 1. Prerrequisitos

### Requisitos en Azure

Un Azure Data Lake Storage Gen2:
- `datadatabrick`

Un contenedor:
- `dev`

Un workspace Azure Databricks con Unity Catalog habilitado

Permisos de Owner en la suscripciÃ³n o Resource Group

### Requisitos en Databricks

- Ser metastore admin
- Tener permisos para crear:
  - Storage Credentials
  - External Data
  - Schemas

---

## âœ… 2. Crear el Access Connector for Azure Databricks

1. Ir al Portal de Azure
2. Crear recurso:
   - **Access Connector for Azure Databricks**
3. Nombre sugerido:
   - `ac-databricks-data`
4. Seleccionar la misma regiÃ³n del workspace
5. Crear

El connector genera una **Managed Identity** que Databricks usarÃ¡ para acceder al Data Lake.

---

## âœ… 3. Asignar permisos RBAC en el ADLS Gen2

1. Ir al Storage Account:
   - **Storage Accounts â†’ datadatabrick â†’ Access Control (IAM) â†’ Add role assignment**

2. Asignar al Access Connector:

### Roles obligatorios
- âœ” Storage Blob Data Contributor
- âœ” Storage Blob Data Reader

### Scope
- âœ” Nivel Storage Account completo

---

## âœ… 4. Crear el Storage Credential en Unity Catalog

En Databricks:

1. **Catalog â†’ Storage Credentials â†’ Create**
2. Nombre:
   - `cred_datadatabrick`
3. Tipo:
   - `Azure Managed Identity`
4. Access connector ID:
   - Copiar desde el portal Azure: en **ac-databricks-data**, copiar el valor que marca **Resource ID**
   - Ejemplo: `/subscriptions/YOUR_SUBSCRIPTION_ID/resourcegroups/YOUR_RESOURCE_GROUP/providers/microsoft.databricks/accessconnectors/ac-databricks-data`
5. Crear

Este credential autoriza UC a usar la Managed Identity para acceder a ADLS.

---

## âœ… 5. Crear la External Data

1. Ir a **Catalog â†’ External Data â†’ Create**
2. Nombre:
   - `extloc_datadatabrick_dev`
3. URL:
   - `abfss://dev@datadatabrick.dfs.core.windows.net/`
4. Storage Credential:
   - `cred_datadatabrick`
5. **Test Connection** â†’ debe ser **Successful**
6. Crear

---

## âœ… 5.5 Crear un CatÃ¡logo en Unity Catalog

Ejecutar en un Notebook SQL:

```sql
CREATE CATALOG IF NOT EXISTS ml_catalog
MANAGED LOCATION 'abfss://dev@datadatabrick.dfs.core.windows.net/ml_catalog';
```

Este catÃ¡logo alojarÃ¡ los esquemas y tablas de tu proyecto de Machine Learning.

---

## âœ… 6. Crear un schema en Unity Catalog asociado al Data Lake

Ejecutar en un Notebook SQL:

```sql
%sql
CREATE SCHEMA IF NOT EXISTS ml_catalog.ml_models
MANAGED LOCATION 'abfss://dev@datadatabrick.dfs.core.windows.net/ml_catalog/ml_models/';
```

---

## âœ… 7. Crear tablas en esa external data

### Celda 1: Leer el archivo CSV desde la ruta

```python
df = spark.read.csv("abfss://dev@dataadatbrick.dfs.core.windows.net/nyc-taxi/", header=True, inferSchema=True)
#display(df)
```

### Celda 2: Escribir el DataFrame como tabla Delta

```python
df.write.format("delta").mode("overwrite").saveAsTable("ml_catalog.ml_models.nyctaxi")
```

---

## ðŸš€ 8. Script SQL Automatizado

```sql
-- Crear Storage Credential (ejecutar como Metastore Admin)
CREATE STORAGE CREDENTIAL cred_datadatabrick
  USING (TYPE = 'AZURE_MANAGED_IDENTITY', AZURE_MANAGED_IDENTITY_ID = '/subscriptions/YOUR_SUBSCRIPTION_ID/resourcegroups/YOUR_RESOURCE_GROUP/providers/microsoft.databricks/accessconnectors/ac-databricks-data');

-- Crear External Data
CREATE EXTERNAL LOCATION extloc_datadatabrick_dev
  URL = 'abfss://dev@datadatabrick.dfs.core.windows.net/'
  WITH (STORAGE_CREDENTIAL = cred_datadatabrick);

-- Crear CatÃ¡logo
CREATE CATALOG IF NOT EXISTS ml_catalog
MANAGED LOCATION 'abfss://dev@datadatabrick.dfs.core.windows.net/ml_catalog';

-- Crear Schema
CREATE SCHEMA IF NOT EXISTS ml_catalog.ml_models
MANAGED LOCATION 'abfss://dev@datadatabrick.dfs.core.windows.net/ml_catalog/ml_models/';

-- Crear Tabla desde CSV
-- Ejecutar en Python:
-- df = spark.read.csv("abfss://dev@dataadatbrick.dfs.core.windows.net/nyc-taxi/", header=True, inferSchema=True)
-- df.write.format("delta").mode("overwrite").saveAsTable("ml_catalog.ml_models.nyctaxi")
```

---

**DocumentaciÃ³n oficial:** [Azure Databricks + ADLS Gen2 + UC](https://docs.microsoft.com/en-us/azure/databricks/)

---

## ðŸŽµ Bonus Track: Comandos Ãštiles

### Comprobar si tu External Location funciona

```sql
DESCRIBE EXTERNAL LOCATION extloc_datadatabrick_dev;
```

### Comando para borrar un catÃ¡logo

```sql
DROP CATALOG IF EXISTS nombre_catalog CASCADE;
```

Reemplaza `nombre_catalog` con el nombre real del catÃ¡logo que deseas eliminar (ej: `ml_catalog`).
